# ./pyspark
Python 2.7.10 |Anaconda 2.3.0 (64-bit)| (default, May 28 2015, 17:02:03)
[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2
Type "help", "copyright", "credits" or "license" for more information.
Anaconda is brought to you by Continuum Analytics.
Please check out: http://continuum.io/thanks and https://binstar.org
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 1.3.1
      /_/

Using Python version 2.7.10 (default, May 28 2015 17:02:03)
SparkContext available as sc, HiveContext available as sqlContext.

>>> a = [('g1', 2), ('g2', 4), ('g3', 3), ('g4', 8)]
>>> a
[('g1', 2), ('g2', 4), ('g3', 3), ('g4', 8)]

>>> rdd = sc.parallelize(a);
>>> rdd.collect()
[('g1', 2), ('g2', 4), ('g3', 3), ('g4', 8)]

>>> sorted = rdd.sortByKey()
>>> sorted.collect()
[('g1', 2), ('g2', 4), ('g3', 3), ('g4', 8)]


>>> rdd2 = rdd.map(lambda (x,y) : (y,x))
>>> rdd2.collect()
[(2, 'g1'), (4, 'g2'), (3, 'g3'), (8, 'g4')]

>>> sorted = rdd2.sortByKey()
>>> sorted.collect()
[(2, 'g1'), (3, 'g3'), (4, 'g2'), (8, 'g4')]


>>> sorted = rdd2.sortByKey(False)
>>> sorted.collect()
[(8, 'g4'), (4, 'g2'), (3, 'g3'), (2, 'g1')]

>>> sorted = rdd2.sortByKey()
>>> sorted.collect()
[(2, 'g1'), (3, 'g3'), (4, 'g2'), (8, 'g4')]
>>>
>>> list
[(2, 'g1'), (3, 'g3'), (4, 'g2'), (8, 'g4')]

>>>
>>> sorted.collect()
[(2, 'g1'), (3, 'g3'), (4, 'g2'), (8, 'g4')]

>>> indices = sorted.zipWithIndex()
>>> indices.collect()
[((2, 'g1'), 0), ((3, 'g3'), 1), ((4, 'g2'), 2), ((8, 'g4'), 3)]
>>>