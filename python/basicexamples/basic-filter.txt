# ./pyspark
Python 2.7.10 |Anaconda 2.3.0 (64-bit)| (default, May 28 2015, 17:02:03)
[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2
Type "help", "copyright", "credits" or "license" for more information.
Anaconda is brought to you by Continuum Analytics.
Please check out: http://continuum.io/thanks and https://binstar.org
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 1.3.1
      /_/

Using Python version 2.7.10 (default, May 28 2015 17:02:03)
SparkContext available as sc, HiveContext available as sqlContext.

>>> sc
<pyspark.context.SparkContext object at 0x10d926210>

>>>
>>> nums = sc.parallelize([1, 2, 3, 4, 5, 6, 7])
>>> nums.collect()
[1, 2, 3, 4, 5, 6, 7]

>>> filtered1 = nums.filter(lambda x : x % 2 == 1)
>>> filtered1.collect()
[1, 3, 5, 7]
>>>
>>> filtered2 = nums.filter(lambda x : x % 2 == 0)
>>> filtered2.collect()
[2, 4, 6]
>>>
