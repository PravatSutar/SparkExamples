# ./pyspark
Python 2.7.10 |Anaconda 2.3.0 (64-bit)| (default, May 28 2015, 17:02:03)
[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2
Type "help", "copyright", "credits" or "license" for more information.
Anaconda is brought to you by Continuum Analytics.
Please check out: http://continuum.io/thanks and https://binstar.org
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 1.3.1
      /_/

Using Python version 2.7.10 (default, May 28 2015 17:02:03)
SparkContext available as sc, HiveContext available as sqlContext.

>>>
>>> nums = [10, 1, 2, 9, 3, 4, 5, 6, 7]
>>> sc.parallelize(nums).takeOrdered(3)
[1, 2, 3]
>>> sc.parallelize(nums).takeOrdered(3, key=lambda x: -x)
[10, 9, 7]
>>>
>>> kv = [(10,"z1"), (1,"z2"), (2,"z3"), (9,"z4"), (3,"z5"), (4,"z6"), (5,"z7"), (6,"z8"), (7,"z9")]
>>> sc.parallelize(kv).takeOrdered(3)
[(1, 'z2'), (2, 'z3'), (3, 'z5')]
>>>
>>> sc.parallelize(kv).takeOrdered(3, key=lambda x: -x[0])
[(10, 'z1'), (9, 'z4'), (7, 'z9')]
