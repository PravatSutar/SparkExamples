1. Prepare Input

# cat data.txt
crazy crazy fox jumped
crazy fox jumped
fox is fast
fox is smart
dog is smart

2. Invoke pyspark
#./pyspark
Python 2.7.10 |Anaconda 2.3.0 (64-bit)| (default, May 28 2015, 17:02:03)
[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2
Type "help", "copyright", "credits" or "license" for more information.
Anaconda is brought to you by Continuum Analytics.
Please check out: http://continuum.io/thanks and https://binstar.org
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 1.3.1
      /_/

Using Python version 2.7.10 (default, May 28 2015 17:02:03)
SparkContext available as sc, HiveContext available as sqlContext.


>>> sc
<pyspark.context.SparkContext object at 0x10ae02210>
>>> lines = sc.textFile("data.txt", 1)
>>> debuglines = lines.collect();
>>> debuglines
[u'crazy crazy fox jumped', 
 u'crazy fox jumped', 
 u'fox is fast', 
 u'fox is smart', 
 u'dog is smart'
]
>>> words = lines.flatMap(lambda x: x.split(' '))
>>> debugwords = words.collect();
>>> debugwords
[
 u'crazy', 
 u'crazy', 
 u'fox', 
 u'jumped', 
 u'crazy', 
 u'fox', 
 u'jumped', 
 u'fox', 
 u'is', 
 u'fast', 
 u'fox', 
 u'is', 
 u'smart', 
 u'dog', 
 u'is', 
 u'smart'
]
>>> ones = words.map(lambda x: (x, 1))
>>> debugones = ones.collect()
>>> debugones
[
 (u'crazy', 1), 
 (u'crazy', 1), 
 (u'fox', 1), 
 (u'jumped', 1), 
 (u'crazy', 1), 
 (u'fox', 1), 
 (u'jumped', 1), 
 (u'fox', 1), 
 (u'is', 1), 
 (u'fast', 1), 
 (u'fox', 1), 
 (u'is', 1), 
 (u'smart', 1), 
 (u'dog', 1), 
 (u'is', 1), 
 (u'smart', 1)
]
>>> counts = ones.reduceByKey(lambda x, y: x + y)
>>> debugcounts = counts.collect()
>>> debugcounts
[
 (u'crazy', 3), 
 (u'jumped', 2), 
 (u'is', 3), 
 (u'fox', 4), 
 (u'dog', 1), 
 (u'fast', 1), 
 (u'smart', 2)
]
>>>
>>> counts.saveAsTextFile("output")

3. Examine Output

# cat output/part*
(u'crazy', 3)
(u'jumped', 2)
(u'is', 3)
(u'fox', 4)
(u'dog', 1)
(u'fast', 1)
(u'smart', 2)
