# ./pyspark
Python 2.7.10 |Anaconda 2.3.0 (64-bit)| (default, May 28 2015, 17:02:03)
[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2
Type "help", "copyright", "credits" or "license" for more information.
Anaconda is brought to you by Continuum Analytics.
Please check out: http://continuum.io/thanks and https://binstar.org
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 1.3.1
      /_/

Using Python version 2.7.10 (default, May 28 2015 17:02:03)
SparkContext available as sc, HiveContext available as sqlContext.

>>> d1= [('k1', 1), ('k2', 2), ('k3', 5)]
>>> d1
[('k1', 1), ('k2', 2), ('k3', 5)]

>>> d2= [('k1', 3), ('k2',4), ('k4', 8)]
>>> d2
[('k1', 3), ('k2', 4), ('k4', 8)]

>>> rdd1 = sc.parallelize(d1)
>>> rdd1.collect()
[('k1', 1), ('k2', 2), ('k3', 5)]

>>> rdd2 = sc.parallelize(d2)
>>> rdd2.collect();
[('k1', 3), ('k2', 4), ('k4', 8)]

>>> rdd3 = rdd1.union(rdd2)
>>> rdd3.collect()
[('k1', 1), ('k2', 2), ('k3', 5), ('k1', 3), ('k2', 4), ('k4', 8)]

>>> rdd4 = rdd3.reduceByKey(lambda x,y: x+y)
>>> rdd4.collect()
[('k3', 5), ('k2', 6), ('k1', 4), ('k4', 8)]